
<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>genesis 联合开发网 - pudn.com</title>

    <link href="/Public/m/css/bootstrap.min.css" rel="stylesheet">
    <link href="/Public/m/font-awesome/css/font-awesome.css" rel="stylesheet">

    <link href="/Public/m/css/animate.css" rel="stylesheet">
    <link href="/Public/m/css/style.css" rel="stylesheet">

	<link href="/Public/m/css/plugins/slick/slick.css" rel="stylesheet">
	<link href="/Public/m/css/plugins/slick/slick-theme.css" rel="stylesheet">

	

	<link href="/css/style.css" rel="stylesheet">

	<!-- Mainly scripts -->
	<script src="/Public/m/js/jquery-2.1.1.js"></script>
	<script src="/Public/m/js/bootstrap.min.js"></script>

	<script src="/Public/m/js/plugins/metisMenu/jquery.metisMenu.js"></script>
	<script src="/Public/m/js/plugins/slimscroll/jquery.slimscroll.min.js"></script>

	<!-- Custom and plugin javascript -->
	<script src="/Public/m/js/inspinia.js"></script>
	<script src="/Public/m/js/plugins/pace/pace.min.js"></script>
	
<META name="Keywords" content="genesis pytorch machine-learning generative-model VAE object-centric 程序员 编程 源码 源代码 下载">

	
<META name="Description" content="genesis GENESIS和GENESIS-V2的官方PyTorch实现">

</head>

<body>

<!-- /.main-container start -->

<div class="pace  pace-inactive">
  	<div class="pace-progress" data-progress-text="100%" data-progress="99" style="transform: translate3d(100%, 0px, 0px);">
  	<div class="pace-progress-inner"></div>
	</div>
	<div class="pace-activity"></div>
</div>

<div class="all-content">
	<div class="my-container top-navigation">
		<nav class="navbar navbar-static-top" role="navigation">
			<div class="navbar-header">
				<button aria-controls="navbar" aria-expanded="false" data-target="#navbar" data-toggle="collapse" class="navbar-toggle collapsed" type="button">
					<i class="fa fa-reorder"></i>
				</button>
				<a href="http://www.pudn.com" class="navbar-brand">联合开发网</a>
			</div>
			<div class="navbar-collapse collapse" id="navbar">
				<ul class="nav navbar-nav">
					<li>
						<a href="/" style="padding-right:8px;padding-left:8px;font-size:16px">首页</a>
					</li>
					<li>
						<a href="https://search.pudn.com" style="padding-right:8px;padding-left:8px;font-size:16px">搜索</a>
					</li>
				</ul>
               	<ul class="nav navbar-top-links navbar-right">
               							<li>
                       	<a href="/User/login.html">
                           	&nbsp; <i class="fa fa-sign-out"></i>登录
                       	</a>
                   	</li>
                   	<li>
                       	<a href="/User/reg.html"><i class="fa fa-sign-out"></i>注册
                       	</a>
                   	</li>
					               	</ul>
			</div>
		</nav>
	</div>
	<form action="https://search.pudn.com/Download" method="get">
	<div class="top-menu-line2">
		<div class="my-container">
			<a href=/Download/upload.html>上传</a>
			&nbsp; <a href=/User>管理</a>
			&nbsp; <a href=https://search.pudn.com/Download>搜索</a>
			&nbsp; <a href=/Guestbook>留言</a>
			<div class="input-group pull-right" style="max-width:200px;margin:3px 15px 0px 0px">
				<input type="text" name="keyword" maxlength=20 placeholder="请输入关键字" class="form-control input-sm">
				<span class="input-group-btn"> <button type="submit" class="btn btn-white input-sm" style="border:1px solid #AAA"><i class="fa fa-search"></i></button>
				</span>
			</div>
		</div>
	</div></form>



<div class="row wrapper white-bg">
	<div class="my-container">
			<div class="nav-dir" style="padding:10px 0px">
				<a href="/">Pudn.com</a>&nbsp;&gt;&nbsp;下载中心
				&nbsp;&gt;&nbsp;内容生成				&nbsp;&gt;&nbsp;<font color=red>genesis</font>
			</div>


		<div class="row">
			<div class="col-xs-12 col-md-8">
				<div class="item-name">genesis</div>
				<div class="item-keyword">
					<a href="#" class="keyword" keyword="pytorch">pytorch</a>&nbsp;<a href="#" class="keyword" keyword="machine-learning">machine-learning</a>&nbsp;<a href="#" class="keyword" keyword="generative-model">generative-model</a>&nbsp;<a href="#" class="keyword" keyword="VAE">VAE</a>&nbsp;<a href="#" class="keyword" keyword="object-centric">object-centric</a>&nbsp;				</div>
				<div class="item-action">
					<div class="btn-group">
						<a href=/Download/dl/id/1686194419119459 target=_blank class="btn btn-primary btn-sm"><i class="fa fa-download"></i> 下载(<span id="down-count">0</span>)</a>
						<a href="###" class="btn btn-white btn-sm vote-up"><i class="fa fa-thumbs-up"></i> 赞(<span id="vote-up-count">87</span>) </a>
						<a href="###" class="btn btn-white btn-sm vote-down"><i class="fa fa-thumbs-down"></i> 踩(<span id="vote-down-count">0</span>) </a>
						<a href="/Download/comment/id/1686194419119459" data-toggle="modal" data-target="#myModal" class="btn btn-white btn-sm"><i class="fa fa-comment"></i> 评论(<span id="comment-count">0</span>) </a>
						<a href="###" class="btn btn-white btn-sm favor-item"><i class="fa fa-heart"></i> 收藏(<span id="favor-count">0</span>) </a>
					</div>
				</div>
				<hr>

				<div class="item-info">
					<B>所属分类</B>：内容生成<BR>
					<B>开发工具</B>：Python<BR>
					<B>文件大小</B>：269KB<BR>
					<B>下载次数</B>：0<BR>
					<B>上传日期</B>：2022-04-13 08:41:26<BR>
					<B>上 传 者</B>：<a href=/User/profile/id/1682139907912860.html>sh-1993</a><BR>
				</div>
				<div class="item-intro">
					说明：&nbsp;&nbsp;GENESIS和GENESIS-V2的官方PyTorch实现<BR>(Official PyTorch implementation of GENESIS and GENESIS-V2)				</div>
				<hr>
				<div><B>文件列表</B>: </div>
				<div id="file-list">
				LICENSE (35149, 2022-04-13)<BR>datasets (0, 2022-04-13)<BR>datasets\apc_config.<B>py</B> (6251, 2022-04-13)<BR>datasets\gqn_config.<B>py</B> (5152, 2022-04-13)<BR>datasets\multi_object_config.<B>py</B> (8286, 2022-04-13)<BR>datasets\multid_config.<B>py</B> (5294, 2022-04-13)<BR>datasets\shapestacks_config.<B>py</B> (6296, 2022-04-13)<BR>datasets\sketchy_config.<B>py</B> (3099, 2022-04-13)<BR>environment.<B>yml</B> (3364, 2022-04-13)<BR>forge (0, 2022-04-13)<BR>models (0, 2022-04-13)<BR>models\__init__.<B>py</B> (0, 2022-04-13)<BR>models\genesis_config.<B>py</B> (18289, 2022-04-13)<BR>models\genesisv2_config.<B>py</B> (10360, 2022-04-13)<BR>models\monet_config.<B>py</B> (7789, 2022-04-13)<BR>models\vae_config.<B>py</B> (3579, 2022-04-13)<BR>modules (0, 2022-04-13)<BR>modules\__init__.<B>py</B> (0, 2022-04-13)<BR>modules\attention.<B>py</B> (9263, 2022-04-13)<BR>modules\blocks.<B>py</B> (6236, 2022-04-13)<BR>modules\component_vae.<B>py</B> (3240, 2022-04-13)<BR>modules\decoders.<B>py</B> (1296, 2022-04-13)<BR>modules\encoders.<B>py</B> (1562, 2022-04-13)<BR>modules\unet.<B>py</B> (3391, 2022-04-13)<BR>scripts (0, 2022-04-13)<BR>scripts\__init__.<B>py</B> (0, 2022-04-13)<BR>scripts\compute_fid.<B>py</B> (5335, 2022-04-13)<BR>scripts\compute_seg_metrics.<B>py</B> (4999, 2022-04-13)<BR>scripts\generate_multid.<B>py</B> (5039, 2022-04-13)<BR>scripts\sketchy_preparation.<B>py</B> (3374, 2022-04-13)<BR>scripts\visualise_data.<B>py</B> (3401, 2022-04-13)<BR>scripts\visualise_generation.<B>py</B> (4596, 2022-04-13)<BR>scripts\visualise_reconstruction.<B>py</B> (4823, 2022-04-13)<BR>third_party (0, 2022-04-13)<BR>third_party\__init__ .<B>py</B> (0, 2022-04-13)<BR>third_party\multi_object_datasets (0, 2022-04-13)<BR>... ...<BR>				</div>
				<hr>
				<div id="readme">
				# Genesis and Genesis-V2

This is the official PyTorch reference implementation of:
> ["GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations"](https://arxiv.org/abs/1907.13052)  
> Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, and Ingmar Posner  
> International Conference on Learning Representations (ICLR), 2020

> ["Reconstruction Bottlenecks in Object-Centric Generative Models"](https://oolworkshop.github.io/program/ool_5.html)  
> Martin Engelcke, Oiwi Parker Jones, and Ingmar Posner  
> Workshop on Object-Oriented Learning at ICML, 2020

> ["GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement"](https://arxiv.org/abs/2104.09958v2)  
> Martin Engelcke, Oiwi Parker Jones, and Ingmar Posner  
> Advances in Neural Information Processing Systems (NeurIPS), 2021

As part of these works, the repository also includes:
* a [re-implementation of MONet](https://github.com/applied-ai-lab/genesis/blob/master/models/monet_config.py) from ["MONet: Unsupervised Scene Decomposition and Representation"](https://arxiv.org/abs/1901.11390) by Burgess et al.;
* a [re-implementation of GECO](https://github.com/applied-ai-lab/genesis/blob/master/utils/geco.py) from ["Taming VAEs"](https://arxiv.org/abs/1810.00597) by Rezende and Viola.

## Setup

### Dependencies
Clone the repository into, e.g., `~/code/genesis`:
```shell
git clone --recursive https://github.com/applied-ai-lab/genesis.git ~/code/genesis
```
We use [Forge](https://github.com/akosiorek/forge) to save some legwork. It is included as a submodule but you need to add it to your python path, e.g. with:
```shell
# If needed, replace .bashrc with .zshrc or similar
echo 'export PYTHONPATH="${PYTHONPATH}:${HOME}/code/genesis/forge"' >> ~/.bashrc
```
You can either install PyTorch, TensorFlow, and all other dependencies manually or you can setup up conda environment with all required dependencies using the `environment.yml` file:
```shell
conda env create -f environment.yml
conda activate genesis_env
```

### Datasets
This repository contains data loaders for the three datasets considered in the [ICLR paper](https://arxiv.org/abs/1907.13052).
A few steps are required for setting up each individual dataset.
We also provide a PyTorch wrapper around the [Multi-Object Datasets](https://github.com/deepmind/multi-object-datasets/) used for the experiments on the `Objects Room` dataset in the [ICML workshop paper](https://oolworkshop.github.io/program/ool_5.html).

#### Multi-dSprites
Generate coloured Multi-dSprites from the original [dSprites dataset](https://github.com/deepmind/dsprites-dataset) with:
```shell
cd ~/code/genesis
mkdir -p data/multi_dsprites/processed
git clone https://github.com/deepmind/dsprites-dataset.git data/multi_dsprites/dsprites-dataset
python scripts/generate_multid.py
```
**NOTE:** An RGB colour is sampled from 125 possible colours for each scene component. By default, multiple components in an image can have the same colour. This can lead, e.g., to a foreground object to have the same colour as the background so that the object is practically "invisible". A ground truth segmentation mask will still be associated with such an invisible object. If you want to avoid this, you can set `--unique_colours True` during training to use an alternative dataset where each component in an image has a unique colour.

#### GQN (rooms-ring-camera)
The [GQN datasets](https://github.com/deepmind/gqn-datasets) are quite large. The `rooms_ring_camera` dataset as used in the paper takes about 250GB and can be downloaded with:
```shell
pip install gsutil
cd ~/code/genesis
mkdir -p data/gqn_datasets
gsutil -m cp -r gs://gqn-dataset/rooms_ring_camera data/gqn_datasets
```
Note that we use a modified version of the TensorFlow GQN data loader from [ogroth/tf-gqn](https://github.com/ogroth/tf-gqn) which is included in `third_party/tf_gqn`.

#### ShapeStacks
You need about 30GB of free disk space for [ShapeStacks](https://shapestacks.robots.ox.ac.uk/):
```shell
# Download compressed dataset
cd data
wget -i ../utils/shapestacks_urls.txt
# Uncompress files
tar xvzf shapestacks-meta.tar.gz
tar xvzf shapestacks-mjcf.tar.gz
tar xvzf shapestacks-rgb.tar.gz
cd -
```
The instance segmentation labels for ShapeStacks can be downloaded from [here](https://drive.google.com/open?id=1KsSQCgb1JJExbKyrIkTwBL9VidGcq2k7).

#### Multi-Object Datasets
The repository contains a wrapper around the [Multi-Object Datasets](https://github.com/deepmind/multi_object_datasets), returning an iterable which behaves similarly to a PyTorch DataLoader object.
The default config assumes that any datasets you wish to use have been downloaded to `data/multi-object-datasets`.
As for the GQN data, this can be done with gsutil.
You can download all four datasets at once with:
```shell
gsutil cp -r gs://multi-object-datasets data/
```

#### Sketchy
Clone [deepmind-research](https://github.com/deepmind/deepmind-research) into, e.g., `code/deepmind-research`:
```shell
git clone https://github.com/deepmind/deepmind-research.git ~/code/deepmind-research
```
Download `lift_green__demos 2` and `stack_green_on_red__demos 2` using the script at `deepmind-research/sketchy/download.sh`.
Put the data into `~/code/genesis/data/sketchy/records` with the contents of the folder being the actual tfrecord files.
Make sure the `deepmind-research` is on your python path:
```shell
# If needed, replace .bashrc with .zshrc or similar
echo 'export PYTHONPATH="${PYTHONPATH}:${HOME}/code/deepmind-research"' >> ~/.bashrc
```
Create a separate environment according to `deepmind-research/sketchy/requirements.txt`
```shell
# Leave current environment first if necessary
conda deactivate
conda create -n sketchy python=3.7
conda activate sketchy
pip install -r ~/code/deepmind-research/sketchy/requirements.txt
# Some additional dependencies
pip install torch==1.3.1 torchvision==0.4.2 tqdm pillow
```
You can now preprocess the data with:
```shell
python scripts/sketchy_preparation.py
conda deactivate
```

#### MIT-Princeton 2016 Amazon Picking Challenge (APC)
Dowload the "Object Segmentation Training Dataset" from the [team's website](http://apc.cs.princeton.edu) via the [download link](http://3dvision.princeton.edu/projects/2016/apc/downloads/training.zip) (ca. 130GB).
Move the `training.zip` file into `~/code/genesis/data/apc` and unpack it.
Preprocess the data by running:
```shell
python datasets/apc_config.py
```

## Training
You can train Genesis-v2, Genesis, MONet and baseline VAEs on the datasets using the default hyperparameters with, e.g.:
```shell
python train.py --data_config datasets/shapestacks_config.py --model_config models/genesisv2_config.py
python train.py --data_config datasets/gqn_config.py --model_config models/genesis_config.py
python train.py --data_config datasets/multi_object_config.py --model_config models/monet_config.py
python train.py --data_config datasets/multid_config.py --model_config models/vae_config.py
```
You can change many of the hyperparameters via the Forge command line flags in the respective config files, e.g.:
```shell
python train.py --data_config datasets/multid_config.py --model_config models/genesis_config.py --batch_size *** --learning_rate 0.001
```
See [train.py](https://github.com/applied-ai-lab/genesis/blob/master/train.py) and the config files for the available flags.

TensorBoard logs are written to file with [TensorboardX](https://github.com/lanpa/tensorboardX). Run `tensorboard --logdir checkpoints` to monitor training.

**NOTE 1:** If you train MONet with the default config flags, then the hyperparameters from our ICLR paper are used which are different from the ones in Burgess et al.. If you want to use the training hyperparameters from Burgess et al., then you need to add the following flags: `--geco False --pixel_std1 0.09 --pixel_std2 0.11 --train_iter 1000000 --batch_size *** --optimiser rmsprop`.

**NOTE 2:** The Sketchy results in the GENESIS-V2 paper use a different GECO goal than used in the other experiments. It is necessary to override the default value to reproduce these results, which can be done by adding `--g_goal 0.5***5` as a training flag.

## Evaluation
To compute the FID score for a trained model you can run, e.g.:
```shell
python scripts/compute_fid.py --data_config datasets/gqn_config.py --model_config models/genesis_config.py --model_dir checkpoints/MyModel/1 --model_file model.ckpt-FINAL
```
Similarly, you can compute the segmentation metrics with, e.g.:
```shell
python scripts/compute_seg_metrics.py --data_config datasets/gqn_config.py --model_config models/genesis_config.py --model_dir checkpoints/MyModel/1 --model_file model.ckpt-FINAL
```

## Visualisation
You can visualise your data with, e.g.:
```shell
python scripts/visualise_data.py --data_config datasets/multid_config.py
python scripts/visualise_data.py --data_config datasets/gqn_config.py
python scripts/visualise_data.py --data_config datasets/shapestacks_config.py
python scripts/visualise_data.py --data_config datasets/multi_object_config.py --dataset objects_room
```
Scripts for visualising reconstructions/segmentations and samples are available at `scripts/visualise_reconstruction.py` and `scripts/visualise_generation.py`, respectively.

## Pretrained models & results
Checkpoints of pretrained models are available [here](https://drive.google.com/drive/folders/1dp6trBGAFUeQ1V3h61Mlv0JeMZqc97sw?usp=sharing).

Generation and segmentation metrics of the released model checkpoints are summarised in the following table:
| Model | Dataset | FID &darr; | ARI-FG &uarr; | MSC-FG &uarr; | ARI &uarr; | MSC &uarr; |
| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |
| GENESIS | Multi-dSprites | 25.0 | 0.57 | 0.69 | - | - |
| GENESIS | GQN | 79.4 | no labels | no labels | no labels | no labels |
| GENESIS | ShapeStacks | 235.4 | 0.71 | 0.*** | - | - |
| GENESIS-V2 | ShapeStacks | 108.1 | 0.80 | 0.66 | - | - |
| GENESIS-V2 | ObjectsRoom | 53.2 | 0.82 | 0.61 | - | - |
| GENESIS-V2 | Sketchy | 212.7 | no labels | no labels | no labels | no labels |
| GENESIS-V2 | APC | 232.2 | - | - | 0.55 | 0.65 |

Other than varying the number of object slots `K`, models are trained with the same default hyperparameters across datasets. Generation and segmentation performance can be improved by further tuning hyperparameters for each individual dataset. For example, [Dang-Nhu & Steger 2021](https://arxiv.org/pdf/2101.04041.pdf) achieve better segmentation performance on Multi-dSprites using smaller standard deviations for the conditional likelihood p(x|z) and a smaller GECO reconstruction target. The authors also achieve good results on CLEVR6 using this implementation with custom hyperparameters.

**NOTE:** Results can vary between individual runs. It is recommended to perform multiple runs with different random seeds to obtain a sense for model performance.

## Further particulars
### License
This source code is licensed under a [GNU General Public License (GPL) v3](https://www.gnu.org/licenses/gpl-3.0.en.html) license, which is included in the [LICENSE](LICENSE) file in the root directory.

### Copyright
Copyright (c) University of Oxford. All rights reserved.

Authors: Applied AI Lab, Oxford Robotics Institute, University of Oxford, https://ori.ox.ac.uk/labs/a2i/

No warranty, explicit or implicit, provided.

### Citation
If you make use of this code in your research, we would appreciate if you considered citing the paper that is most relevant to your work:
```
@inproceedings{engelcke2020genesis,
  title={{GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations}},
  author={Engelcke, Martin and Kosiorek, Adam R and Parker Jones, Oiwi and Posner, Ingmar},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}
@article{engelcke2020reconstruction,
  title={{Reconstruction Bottlenecks in Object-Centric Generative Models}},
  author={Engelcke, Martin and Parker Jones, Oiwi and Posner, Ingmar},
  journal={ICML Workshop on Object-Oriented Learning},
  year={2020}
}
@article{engelcke2021genesisv2,
  title={{GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement}},
  author={Engelcke, Martin and Parker Jones, Oiwi and Posner, Ingmar},
  journal={arXiv preprint arXiv:2104.09958},
  year={2021}
}
```

### Third party code
This repository builds upon code from the following third party repositories, which are included in the [third_party](third_party) folder:
- [tf-gqn](https://github.com/ogroth/tf-gqn) (Apache v2 license)
- [shapestacks](https://github.com/ogroth/shapestacks) (GPL v3.0)
- [sylvester-flows](https://github.com/riannevdberg/sylvester-flows) (MIT license)
- [pytorch-fid](https://github.com/mseitzer/pytorch-fid) (Apache v2 license)
- [multi_object_datasets](https://github.com/deepmind/multi_object_datasets) (Apache v2 license)

The full licenses are included in the respective folders.

### Contributions

We welcome contributions via pull requests.
Otherwise, drop us a line if you come across any issues or to request additional features.
				</div>
				<hr>
				<div><B>近期下载者</B>：</div>
				<div id="download-users"></div>
				<hr>
				<div><B>相关文件</B>：</div>
				<div id="relate-items"></div>
				<hr>
				<div><B>评论</B>：[<a href=/Download/comment/id/1686194419119459.html data-toggle=modal data-target="#myModal">我要评论</a>]&nbsp;[<a class='pop-a' href=/Download/report/id/1686194419119459.html>举报此文件</a>]</div>
				<div id="file-comments"></div>
				<hr>
				<div><B>收藏者</B>：</div>
				<div id="favor-users"></div>
				<p></p>
			</div>

			<div class="col-xs-12 col-md-4">

				<div class="ad-sidebar text-center">
					<div class="ad-300">
					</div>
				</div>
			</div>
		</div>

	</div>
</div>



	<div class="my-footer">
		<div class="container">
		<div class="pull-right">
		</div>

		<div>
			
			<a href="http://www.pudn.com" target=_blank>© 联合开发网 from 2004</a> | 
			<a href="/Index/contact.html">联系站长</a> | 
			<a href=" https://beian.miit.gov.cn" target=_blank>湘ICP备2023001425号</a> | 
			<a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=43010502000604" target=_blank>网安备43010502000604</a> | 
		</div>
		</div>
	</div>


</div><!-- /wrapper-->




<!-- page specific plugin scripts -->

<!-- inline scripts related to this page -->

<div id="myModal" class="modal fade" tabindex="-1" 
			role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			
		</div>
	</div>
</div><!-- /.modal-table -->

</body>

<script type="text/javascript">
$(document).on('click', '.list-more', function(){
	var id=$(this).attr('data-id');
	$('#list-'+id).css('max-height',$('#list-'+id)[0].scrollHeight);
	$(this).removeClass('list-more');
	$(this).addClass('list-hide');
	$(this).html('<i class="fa fa-angle-double-up"></i>');
	
	//$(this).hide();

	return false;
});

$(document).on('click', '.list-hide', function(){
	var id=$(this).attr('data-id');
	$('#list-'+id).css('max-height','100px');
	$(this).removeClass('list-hide');
	$(this).addClass('list-more');
	$(this).html('<i class="fa fa-angle-double-down"></i>');
	
	return false;
});

$("#myModal").on("hidden.bs.modal", function() {
    $(this).removeData();
});

$(document).on("click",".keyword",function(){
	var keyword=$(this).attr("keyword");
	var type_id=$(this).attr("type_id");
	if(typeof(type_id) =="undefined" || type_id =="") type_id="0";
	location.href="http://search.pudn.com/Download/index?keyword="+keyword;
	
	return false;
});
</script>
<script type="text/javascript" src="/js/time.js"></script>


<script type="text/javascript" src="/js/marked.min.js"></script>
<script type="text/javascript">
	$(document).ready(function(){
		$('.hide-list').each(function(){
			if ($(this)[0].offsetHeight < $(this)[0].scrollHeight){
				var id=$(this).attr('id');
				id =id.substring(5);
				$(this).after('<div style="text-align:center"><a href="" class="list-more" data-id="' + id + '"><i class="fa fa-angle-double-down"></i></a></div>');
			}
		});
	});
	var g_id="1686194419119459";
	var keywords =new Array();
	keywords[0] ='pytorch';keywords[1] ='machine-learning';keywords[2] ='generative-model';keywords[3] ='VAE';keywords[4] ='object-centric';	function get_download_user(){
		var url ="/Download/get_download_user/id/"+g_id+".html";
		$.get(url,function(ret){
			if(ret.length ==0) return;
			var html='';
			for(i in ret){
				html =html + '<a href=/User/profile/id/'+ret[i].user_new_id+'.html>'+ret[i].name+'</a> ';
			}
			$('#download-users').append(html);
		});
	}
	
	function get_relate_item(){
		var url ="/Download/get_relate_item/id/"+g_id+'.html';
		$.get(url,function(ret){
			if(ret.length ==0) return;
			var html='';
			var intro;
			for(i in ret){
				intro =ret[i].intro;
				for(j in keywords){
					var reg =new RegExp(keywords[j],'gmi');
					intro =intro.replace(reg,'<font color=brown>'+keywords[j]+'</font>');
				}
				html =html + '[<a href=/Download/item/id/'+ret[i].new_id+'.html>'+ret[i].name+'</a>]&nbsp; '+intro+'<BR>';
			}
			$('#relate-items').append(html);
		});
		
	}
	function get_score_name(score){
		switch(score){
		case '100': return '很好，推荐下载';
		case '85': return '还不错';
		case '75': return '一般，勉强可用';
		case '50': return '差';
		case '3': return '纯粹是垃圾';
		case '40': return '和说明完全不符';
		case '20': return '文件不全';
		case '10': return '不是源代码或资料';
		case '5': return '文件有密码，不知道密码';
		case '0': return '不能解压或下载失败';
		}
		return '';
	}
	function get_comment(){
		var url ="/Download/get_comments/id/"+g_id+'.html';
		$.get(url,function(ret){
			if(ret.length ==0) return;
			var total_count =ret.total_count;
			var data =ret.data;
			var html='';
			for(i in data){
				html =html + '<a href="/User/profile/id/'+data[i].user_new_id+'.html" class="uploader">'+data[i].user_name+'</a>: <span class="comment-score">'+get_score_name(data[i].score)+'</span>, '+data[i].content+'<BR>';
			}
			$('#file-comments').append(html);
		});
		
	}
	function get_favor(){
		var url ="/Download/get_item_favors/id/"+g_id+".html";
		$.get(url,function(ret){
			if(ret.length ==0) return;
			var html='';
			for(i in ret){
				html =html + '<a href=/User/profile/id/'+ret[i].user_new_id+'.html class=user>'+ret[i].name+'</a>&nbsp;';
			}
			$('#favor-users').html(html);
		});
	}
	// 得到下载这个的用户又下载了什么
	function get_more_download(){
		
	}
	// 得到下载这个的用户又搜索了什么
	function get_more_keyword(){
		
	}
	
	// 得到论坛相关问题
	function get_bbs(){
		
	}
	
	// 得到软件工场相关内容
	function get_works(){
		
	}
	
	// 得到相关聊天室
	function get_chat(){
		
	}
	
	// 得到相关软件商城信息
	function get_shop(){
		
	}
	
	// 得到job
	
	// 得到学习内容

	// 数据
	get_download_user();
	//get_relate_item();
	get_comment();
	get_favor();
	
	$('.vote-up').click(function(){
		var url="/Download/vote/t/up/id/"+g_id;
		$.get(url,function(ret){
			if(ret.status==0){
				alert(ret.info);
				if(ret.url.length >0)
					location.href=ret.url;
			}
			else{
				var s =$('#vote-up-count').html();
				if(s =='') s="0";
				var count =parseInt(s)+1;
				$('#vote-up-count').html(count);
			}
		})
		return false;
	});
	$('.vote-down').click(function(){
		var url="/Download/vote/t/down/id/"+g_id;
		$.get(url,function(ret){
			if(ret.status==0)
				alert(ret.info);
			else{
				var s =$('#vote-down-count').html();
				if(s =='') s="0";
				var count =parseInt(s)+1;
				$('#vote-down-count').html(count);
			}
		})
		return false;
	});
	$('.favor-item').click(function(){
		var url="/Favor/add/t/0/id/"+g_id;
		$.get(url,function(ret){
			if(ret.status==0){
				alert(ret.info);
			}
			else{
				var s =$('#favor-count').html();
				if(s =='') s="0";
				var count =parseInt(s)+1;
				$('#favor-count').html(count);
			}
		})
		return false;
	});
	$('.pop-a').click(function(){
		var url=$(this).attr('href');
		$.get(url,function(ret){
			alert(ret.info);
			//location.reload();
		})
		return false;
	});

document.getElementById('readme').innerHTML =marked.parse(document.getElementById('readme').innerHTML);
</script>



</html>