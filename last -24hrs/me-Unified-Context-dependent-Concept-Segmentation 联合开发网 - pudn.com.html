
<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>me-Unified-Context-dependent-Concept-Segmentation 联合开发网 - pudn.com</title>

    <link href="/Public/m/css/bootstrap.min.css" rel="stylesheet">
    <link href="/Public/m/font-awesome/css/font-awesome.css" rel="stylesheet">

    <link href="/Public/m/css/animate.css" rel="stylesheet">
    <link href="/Public/m/css/style.css" rel="stylesheet">

	<link href="/Public/m/css/plugins/slick/slick.css" rel="stylesheet">
	<link href="/Public/m/css/plugins/slick/slick-theme.css" rel="stylesheet">

	

	<link href="/css/style.css" rel="stylesheet">

	<!-- Mainly scripts -->
	<script src="/Public/m/js/jquery-2.1.1.js"></script>
	<script src="/Public/m/js/bootstrap.min.js"></script>

	<script src="/Public/m/js/plugins/metisMenu/jquery.metisMenu.js"></script>
	<script src="/Public/m/js/plugins/slimscroll/jquery.slimscroll.min.js"></script>

	<!-- Custom and plugin javascript -->
	<script src="/Public/m/js/inspinia.js"></script>
	<script src="/Public/m/js/plugins/pace/pace.min.js"></script>
	
<META name="Keywords" content="me-Unified-Context-dependent-Concept-Segmentation 程序员 编程 源码 源代码 下载">

	
<META name="Description" content="me-Unified-Context-dependent-Concept-Segmentation The summary of code and paper for unified model towards context-dependent (CD) concept segmentation. , stars:7, update:2024-06-14 09:58:33">

</head>

<body>

<!-- /.main-container start -->

<div class="pace  pace-inactive">
  	<div class="pace-progress" data-progress-text="100%" data-progress="99" style="transform: translate3d(100%, 0px, 0px);">
  	<div class="pace-progress-inner"></div>
	</div>
	<div class="pace-activity"></div>
</div>

<div class="all-content">
	<div class="my-container top-navigation">
		<nav class="navbar navbar-static-top" role="navigation">
			<div class="navbar-header">
				<button aria-controls="navbar" aria-expanded="false" data-target="#navbar" data-toggle="collapse" class="navbar-toggle collapsed" type="button">
					<i class="fa fa-reorder"></i>
				</button>
				<a href="http://www.pudn.com" class="navbar-brand">联合开发网</a>
			</div>
			<div class="navbar-collapse collapse" id="navbar">
				<ul class="nav navbar-nav">
					<li>
						<a href="/" style="padding-right:8px;padding-left:8px;font-size:16px">首页</a>
					</li>
					<li>
						<a href="https://search.pudn.com" style="padding-right:8px;padding-left:8px;font-size:16px">搜索</a>
					</li>
				</ul>
               	<ul class="nav navbar-top-links navbar-right">
               							<li>
                       	<a href="/User/login.html">
                           	&nbsp; <i class="fa fa-sign-out"></i>登录
                       	</a>
                   	</li>
                   	<li>
                       	<a href="/User/reg.html"><i class="fa fa-sign-out"></i>注册
                       	</a>
                   	</li>
					               	</ul>
			</div>
		</nav>
	</div>
	<form action="https://search.pudn.com/Download" method="get">
	<div class="top-menu-line2">
		<div class="my-container">
			<a href=/Download/upload.html>上传</a>
			&nbsp; <a href=/User>管理</a>
			&nbsp; <a href=https://search.pudn.com/Download>搜索</a>
			&nbsp; <a href=/Guestbook>留言</a>
			<div class="input-group pull-right" style="max-width:200px;margin:3px 15px 0px 0px">
				<input type="text" name="keyword" maxlength=20 placeholder="请输入关键字" class="form-control input-sm">
				<span class="input-group-btn"> <button type="submit" class="btn btn-white input-sm" style="border:1px solid #AAA"><i class="fa fa-search"></i></button>
				</span>
			</div>
		</div>
	</div></form>



<div class="row wrapper white-bg">
	<div class="my-container">
			<div class="nav-dir" style="padding:10px 0px">
				<a href="/">Pudn.com</a>&nbsp;&gt;&nbsp;下载中心
				&nbsp;&gt;&nbsp;collect				&nbsp;&gt;&nbsp;<font color=red>me-Unified-Context-dependent-Concept-Segmentation</font>
			</div>


		<div class="row">
			<div class="col-xs-12 col-md-8">
				<div class="item-name">me-Unified-Context-dependent-Concept-Segmentation</div>
				<div class="item-keyword">
									</div>
				<div class="item-action">
					<div class="btn-group">
						<a href=/Download/dl/id/1718495935929134 target=_blank class="btn btn-primary btn-sm">去下载(<span id="down-count">0</span>)</a>
						<a href="###" class="btn btn-white btn-sm vote-up"><i class="fa fa-thumbs-up"></i> 赞(<span id="vote-up-count">0</span>) </a>
						<a href="###" class="btn btn-white btn-sm vote-down"><i class="fa fa-thumbs-down"></i> 踩(<span id="vote-down-count">0</span>) </a>
						<a href="/Download/comment/id/1718495935929134" data-toggle="modal" data-target="#myModal" class="btn btn-white btn-sm"><i class="fa fa-comment"></i> 评论(<span id="comment-count">0</span>) </a>
						<a href="###" class="btn btn-white btn-sm favor-item"><i class="fa fa-heart"></i> 收藏(<span id="favor-count">0</span>) </a>
					</div>
				</div>
				<hr>

				<div class="item-info">
					<B>所属分类</B>：collect<BR>
					<B>开发工具</B>：Python<BR>
					<B>文件大小</B>：0KB<BR>
					<B>下载次数</B>：0<BR>
					<B>上传日期</B>：2024-06-16 07:58:55<BR>
					<B>上 传 者</B>：<a href=/User/profile/id/1682139907912860.html>sh-1993</a><BR>
				</div>
				<div class="item-intro">
					说明：&nbsp;&nbsp;The summary of code and paper for unified model towards context-dependent (CD) concept segmentation. , stars:7, update:2024-06-14 09:58:33				</div>
				<hr>
				<div><B>文件列表</B>: </div>
				<div id="file-list">
				image/<BR>				</div>
				<hr>
				<div id="readme">
				# Awesome-UniCDSeg-Segmentation


<p align="center">
    <img src="image/CD_branch.png"/> <br />
</p>

# <p align=center>`Awesome List for Unified Context-dependent Concept Segmentation (UniCDSeg)`

![Badge](https://img.shields.io/badge/-As%20awesome%20as%20you%20think!-red)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)


This awesome list is under construction. If you have anything to recommend or any suggestions, please feel free to contact us via e-mail (zxq@mail.dlut.edu.cn) or directly push a PR. 

< **Last updated: 30/05/2024** >


##  1. Content


- [Awesome-UniCDSeg-Segmentation](#awesome-unicdseg-segmentation)
- [`Awesome List for Unified Context-dependent Concept Segmentation (UniCDSeg)`](#awesome-list-for-unified-context-dependent-concept-segmentation-unicdseg)
  - [1. Content](#1-content)
  - [2. CI Concept vs. CD Concept](#2-ci-concept-vs-cd-concept)
    - [Definition of CI and CD Concepts](#definition-of-ci-and-cd-concepts)
    - [Current Status of Development of CI and CD Concept Segmentation](#current-status-of-development-of-ci-and-cd-concept-segmentation)
  - [3. Diverse CD Concept Datasets (Commonly used in evaluation)](#3-diverse-cd-concept-datasets-commonly-used-in-evaluation)
  - [4.  Evaluation Metrics](#4--evaluation-metrics)
  - [5. Paper List](#5-paper-list)
    - [5.1. Survey](#51-survey)
    - [5.2. Methods](#52-methods)

		
##  2. CI Concept vs. CD Concept
<p align="center">
    <img src="image/CI_vs_CD.png"/> <br />
</p>

### Definition of CI and CD Concepts  
-  In philosophy and cognitive science, concepts usually contain the context-independent (CI) and context-dependent (CD) concepts, as described by the psychologist and cognitive scientist [Barsalou](https://core.ac.uk/download/pdf/205745705.pdf).
- The most important difference between context-dependent (CD) and context-independent (CI) concepts is whether the background environment (spatial context) plays a decisive role in the definition of the concept. The CD concepts mean that the target is not cognizable without its background environment. But, the context-independent (CI) concepts still have its clear definition even without a background environment.
- The CI concept such as bear and bus can be well understood by relying only on the foreground. While the CD concept is the complete opposite, salient and shadow objects require a specific background to highlight the expression of saliency and shadow concepts. Similarly, it is impossible to determine whether the lesions are polyp or COVID-19 infection without background information. Just because of this property, we need to ensure the semantic certainty of visual prompts when unifying modeling.
### Current Status of Development of CI and CD Concept Segmentation
- For the CI concept segmentation field (that is, semantic segmentation), popular datasets have multiple concept annotations for a single image. The trained CI models can well distinguish different concepts in a unified model.
- For the CD concept segmentation field, existing works explore the in-domain modeling, resulting in repetitive structure design, inefficient data utilization, and limited multi-domain generalization. And, the CI concept unified models cannot well solve the CD concept tasks.

## 3. Diverse CD Concept Datasets (Commonly used in evaluation)
| **Dataset Name** | CD Concept Type|**Year** | **Publication** | **Links** |
| :------: | :------: | :------:| :-------: | :---------|
[DUTS](https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Learning_to_Detect_CVPR_2017_paper.pdf) |Salient Object Segmentation| 2017 | CVPR | [Paper](https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Learning_to_Detect_CVPR_2017_paper.pdf) |
[DUT-OMRON](https://openaccess.thecvf.com/content_cvpr_2013/papers/Yang_Saliency_Detection_via_2013_CVPR_paper.pdf) |Salient Object Segmentation| 2013 |CVPR| [Paper](https://openaccess.thecvf.com/content_cvpr_2013/papers/Yang_Saliency_Detection_via_2013_CVPR_paper.pdf) |
[PASCAL-S](https://arxiv.org/pdf/1406.2807) |Salient Object Segmentation| 2014 | CVPR | [Paper](https://arxiv.org/pdf/1406.2807) |
[HKU-IS](https://openaccess.thecvf.com/content_cvpr_2015/papers/Li_Visual_Saliency_Based_2015_CVPR_paper.pdf) |Salient Object Segmentation| 2015 | CVPR | [Paper](https://openaccess.thecvf.com/content_cvpr_2015/papers/Li_Visual_Saliency_Based_2015_CVPR_paper.pdf) |
[ECSSD](https://openaccess.thecvf.com/content_cvpr_2013/papers/Yan_Hierarchical_Saliency_Detection_2013_CVPR_paper.pdf) |Salient Object Segmentation| 2015 |CVPR| [Paper](https://openaccess.thecvf.com/content_cvpr_2013/papers/Yan_Hierarchical_Saliency_Detection_2013_CVPR_paper.pdf)|
[CHAMELEON](https://www.polsl.pl/rau6/chameleon-database-animal-camouflage-analysis/) |Camouflaged Object Segmentation | 2018 | - | [Paper](https://www.polsl.pl/rau6/chameleon-database-animal-camouflage-analysis/) |
[CAMO](https://arxiv.org/pdf/2105.09451) |Camouflaged Object Segmentation| 2019 |CVIU| [Paper](https://arxiv.org/pdf/2105.09451) |
[COD10K](https://openaccess.thecvf.com/content_CVPR_2020/papers/Fan_Camouflaged_Object_Detection_CVPR_2020_paper.pdf) |Camouflaged Object Segmentation| 2020 | CVPR | [Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Fan_Camouflaged_Object_Detection_CVPR_2020_paper.pdf) |
[NC4K](https://openaccess.thecvf.com/content/CVPR2021/papers/Lv_Simultaneously_Localize_Segment_and_Rank_the_Camouflaged_Objects_CVPR_2021_paper.pdf) |Camouflaged Object  Segmentation| 2021 | CVPR | [Paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Lv_Simultaneously_Localize_Segment_and_Rank_the_Camouflaged_Objects_CVPR_2021_paper.pdf) |
[UCF](https://ieeexplore.ieee.org/document/5540209) |Shadow Segmentation| 2010 |CVPR| [Paper](https://ieeexplore.ieee.org/document/5540209) |
[ISTD](https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Stacked_Conditional_Generative_CVPR_2018_paper.pdf) |Shadow Segmentation| 2018 | CVPR | [Paper](https://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Stacked_Conditional_Generative_CVPR_2018_paper.pdf) |
[SBU](https://link.springer.com/chapter/10.1007/978-3-319-46466-4_49) |Shadow Segmentation| 2016 | ECCV | [Paper](https://link.springer.com/chapter/10.1007/978-3-319-46466-4_49) |
[Trans10K](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580681.pdf) |Transparent Object Segmentation| 2020 | ECCV | [Paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580681.pdf) |
[Trans10Kv2](https://arxiv.org/pdf/2101.08461) |Transparent Object Segmentation| 2021 | IJCAI | [Paper](https://arxiv.org/pdf/2101.08461) |
[ETIS-LaribPolypDB](https://link.springer.com/article/10.1007/s11548-013-0926-3) |Colonoscopy Polyp Segmentation| 2014 | IJCARS | [Paper](https://link.springer.com/article/10.1007/s11548-013-0926-3) |
[CVC-ColonDB](https://xueliancheng.github.io/SLT-Net-project/) |Colonoscopy Polyp Segmentation| 2015 | TMI | [Paper](https://link.springer.com/article/10.1007/s11548-013-0926-3) |
[CVC-ClinicDB](https://polyp.grand-challenge.org/CVCClinicDB/) |Colonoscopy Polyp Segmentation| 2015 | CMIG | [Paper](https://www.sciencedirect.com/science/article/pii/S0895611115000567) |
[Kvasir](https://datasets.simula.no/kvasir/) |Colonoscopy Polyp Segmentation| 2017 | ACM MMSys | [Paper](https://dl.acm.org/doi/abs/10.1145/3083187.3083212) |
[CVC-300](https://github.com/jbernoz/deeppolyp) |Colonoscopy Polyp Segmentation| 2017 |JHE | [Paper](https://www.hindawi.com/journals/jhe/2017/4037190/)|
[COVID-19 data](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9098956) |COVID-19 Lung Infection| 2020 |TMI | [Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9098956)|
[BUSI](https://www.sciencedirect.com/science/article/pii/S2352340919312181) |Breast Lesion Segmentation| 2020 |Data in Brief | [Paper](https://www.sciencedirect.com/science/article/pii/S2352340919312181)|
[ISIC17-20](https://challenge.isic-archive.com/data/) |Skin Lesion Segmentation| 2017-2020 | -| [Website](https://challenge.isic-archive.com/data/)|

##  4.  Evaluation Metrics
[PySegMetrics (PSM): A Python-based Simple yet Efficient Evaluation Toolbox for Segmentation-like tasks](https://github.com/Xiaoqi-Zhao-DLUT/PySegMetric_EvalToolkit)

- **Pixel Accuracy (PA)** is calculated based on
the binarized prediction mask and ground-truth:
<p align="center">
    <img src="image/PA_metric.png" width=40%/> <br />
</p>

- **F-measure** is a metric that comprehensively considers both precision and recall:
<p align="center">
    <img src="image/Fm_metric.png" width=40%/> <br />
</p>

- **weighted F-measure** is proposed to improve the metric F-measure. It assigns different weights (ω) to precision and recall across different errors at different locations, considering the neighborhood information:
<p align="center">
    <img src="image/Fm_metric.png" width=40%/> <br />
</p>

- **S-measure** evaluates the spatial structure similarity by combining the region-aware
structural similarity Sr and the object-aware structural similarity So:
<p align="center">
    <img src="image/Sm_metric.png" width=40%/> <br />
</p>

- **E-measure**  can jointly capture image level statistics and local pixel matching information:
<p align="center">
    <img src="image/Em_metric.png" width=40%/> <br />
</p>

- **IOU** is the most common metric for evaluating classification accuracy:
<p align="center">
    <img src="image/IOU_metric.png" width=40%/> <br />
</p>

- **Dice** is a statistic used to gauge the similarity of two samples and become the most used metric in validating medical image segmentation:
<p align="center">
    <img src="image/DICE_metric.png" width=40%/> <br />
</p>

- **Balanced error rate (BER)** is a common metric to evaluate shadow detection performance, where shadow and non-shadow regions contribute equally to the overall performance without considering their relative areas:
<p align="center">
    <img src="image/BER_metric.png" width=40%/> <br />
</p>


- **MAE** measures the average absolute difference between the prediction and the ground truth pixel by pixel:
<p align="center">
    <img src="image/MAE_metric.png" width=40%/> <br />
</p>


##  5. Paper List
###  5.1. Survey
| **Year** | **Model** | **Publication** | **Title**                                 |  **Links**                                                    |
| :------: | :------: |:------: | :----------------------------------------------------------- |  :----------------------------------------------------------- |
| 2024 | GateNetv2 | IJCV |Towards Diverse Binary Segmentation via A Simple yet General Gated Network <br><sup><sub>Xiaoqi Zhao; Youwei Pang; Lihe Zhang; Huchuan Lu; Lei Zhang</sub></sup> | [Paper](https://arxiv.org/pdf/2303.10396)/[Code](https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency)
###  5.2. Methods
| **Year** | **Model** | **Publication** | **Title**                                 |  **Links**                                                    |
| :------: | :------: |:------: | :----------------------------------------------------------- |  :----------------------------------------------------------- |
| 2024 | Universal Model | MIA | Universal and Extensible Language-Vision Models for Organ Segmentation and Tumor Detection from Abdominal Computed Tomography<br><sup><sub>Jie Liu, Yixiao Zhang, Kang Wang, Mehmet Can Yavuz, Xiaoxi Chen, Yixuan Yuan, Haoliang Li, Yang Yang, Alan Yuille, Yucheng Tang, Zongwei Zhou </sub></sup> | [Paper](https://arxiv.org/pdf/2405.18356)/[Code](https://github.com/ljwztc/CLIP-Driven-Universal-Model)
| 2024 | Spider | ICML | Spider : A Unified Framework for Context-dependent Concept Segmentation<br><sup><sub>Xiaoqi Zhao; Youwei Pang; Wei Ji; Baicheng Sheng; Jiaming Zuo; Lihe Zhang; Huchuan Lu </sub></sup> | [Paper](https://arxiv.org/pdf/2405.01002)/[Code](https://github.com/Xiaoqi-Zhao-DLUT/Spider-UniCDSeg)
| 2024 | GateNetv2 | IJCV |Towards Diverse Binary Segmentation via A Simple yet General Gated Network <br><sup><sub>Xiaoqi Zhao; Youwei Pang; Lihe Zhang; Huchuan Lu; Lei Zhang</sub></sup> | [Paper](https://arxiv.org/pdf/2303.10396)/[Code](https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency)
| 2024 | VSCode | CVPR | VSCode: General Visual Salient and Camouflaged Object Detection with 2D Prompt Learning <br><sup><sub>Ziyang Luo; Nian Liu; Wangbo Zhao; Xuguang Yang; Dingwen Zhang; Deng-Ping Fan; Fahad Khan; Junwei Han</sub></sup> | [Paper](https://arxiv.org/pdf/2311.15011)/[Code](https://github.com/Sssssuperior/VSCode)
| 2023 | SegGPT | ICCV | SegGPT: Towards Segmenting Everything in Context <br><sup><sub>Xinlong Wang;  Xiaosong Zhang; Yue Cao; Wen Wang; Chunhua Shen; Tiejun Huang</sub></sup> | [Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_SegGPT_Towards_Segmenting_Everything_in_Context_ICCV_2023_paper.pdf)/[Code](https://github.com/baaivision/Painter)
| 2023 | UniverSeg | ICCV | UniverSeg: Universal Medical Image Segmentation <br><sup><sub>Victor Ion Butoi;  Jose Javier Gonzalez Ortiz; Tianyu Ma; Mert R. Sabuncu; John Guttag; Adrian V. Dalca</sub></sup> | [Paper](https://arxiv.org/pdf/2304.06131)/[Code](https://github.com/JJGO/UniverSeg)
| 2023 | EVP | CVPR | Explicit Visual Prompting for Low-Level Structure Segmentations <br><sup><sub>Weihuang Liu; Xi Shen; Chi-Man Pun; Xiaodong Cun</sub></sup> | [Paper](https://arxiv.org/pdf/2303.10883)/[Code](https://github.com/NiFangBaAGe/Explicit-Visual-Prompt)
| 2023 | M2SNet | arXiv | M2SNet: Multi-scale in Multi-scale Subtraction Network for Medical Image Segmentation <br><sup><sub>Xiaoqi Zhao; Hongpeng Jia; Youwei Pangl; Long Lv; Feng Tian; Lihe Zhang; Weibing Sun; Huchuan Lu</sub></sup> | [Paper](https://arxiv.org/pdf/2303.10894)/[Code](https://github.com/Xiaoqi-Zhao-DLUT/MSNet-M2SNet)

				</div>
				<hr>
				<div><B>近期下载者</B>：</div>
				<div id="download-users"></div>
				<hr>
				<div><B>相关文件</B>：</div>
				<div id="relate-items"></div>
				<hr>
				<div><B>评论</B>：[<a href=/Download/comment/id/1718495935929134.html data-toggle=modal data-target="#myModal">我要评论</a>]&nbsp;[<a class='pop-a' href=/Download/report/id/1718495935929134.html>举报此文件</a>]</div>
				<div id="file-comments"></div>
				<hr>
				<div><B>收藏者</B>：</div>
				<div id="favor-users"></div>
				<p></p>
			</div>

			<div class="col-xs-12 col-md-4">

				<div class="ad-sidebar text-center">
					<div class="ad-300">
					</div>
				</div>
			</div>
		</div>

	</div>
</div>



	<div class="my-footer">
		<div class="container">
		<div class="pull-right">
		</div>

		<div>
			
			<a href="http://www.pudn.com" target=_blank>© 联合开发网 from 2004</a> | 
			<a href="/Index/contact.html">联系站长</a> | 
			<a href=" https://beian.miit.gov.cn" target=_blank>湘ICP备2023001425号</a> | 
			<a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=43010502000604" target=_blank>网安备43010502000604</a> | 
		</div>
		</div>
	</div>


</div><!-- /wrapper-->




<!-- page specific plugin scripts -->

<!-- inline scripts related to this page -->

<div id="myModal" class="modal fade" tabindex="-1" 
			role="dialog" aria-labelledby="myModalLabel" aria-hidden="true">
	<div class="modal-dialog">
		<div class="modal-content">
			
		</div>
	</div>
</div><!-- /.modal-table -->

</body>

<script type="text/javascript">
$(document).on('click', '.list-more', function(){
	var id=$(this).attr('data-id');
	$('#list-'+id).css('max-height',$('#list-'+id)[0].scrollHeight);
	$(this).removeClass('list-more');
	$(this).addClass('list-hide');
	$(this).html('<i class="fa fa-angle-double-up"></i>');
	
	//$(this).hide();

	return false;
});

$(document).on('click', '.list-hide', function(){
	var id=$(this).attr('data-id');
	$('#list-'+id).css('max-height','100px');
	$(this).removeClass('list-hide');
	$(this).addClass('list-more');
	$(this).html('<i class="fa fa-angle-double-down"></i>');
	
	return false;
});

$("#myModal").on("hidden.bs.modal", function() {
    $(this).removeData();
});

$(document).on("click",".keyword",function(){
	var keyword=$(this).attr("keyword");
	var type_id=$(this).attr("type_id");
	if(typeof(type_id) =="undefined" || type_id =="") type_id="0";
	location.href="http://search.pudn.com/Download/index?keyword="+keyword;
	
	return false;
});
</script>
<script type="text/javascript" src="/js/time.js"></script>


<script type="text/javascript" src="/js/marked.min.js"></script>
<script type="text/javascript">
	$(document).ready(function(){
		$('.hide-list').each(function(){
			if ($(this)[0].offsetHeight < $(this)[0].scrollHeight){
				var id=$(this).attr('id');
				id =id.substring(5);
				$(this).after('<div style="text-align:center"><a href="" class="list-more" data-id="' + id + '"><i class="fa fa-angle-double-down"></i></a></div>');
			}
		});
	});
	var g_id="1718495935929134";
	var keywords =new Array();
		function get_download_user(){
		var url ="/Download/get_download_user/id/"+g_id+".html";
		$.get(url,function(ret){
			if(ret.length ==0) return;
			var html='';
			for(i in ret){
				html =html + '<a href=/User/profile/id/'+ret[i].user_new_id+'.html>'+ret[i].name+'</a> ';
			}
			$('#download-users').append(html);
		});
	}
	
	function get_relate_item(){
		var url ="/Download/get_relate_item/id/"+g_id+'.html';
		$.get(url,function(ret){
			if(ret.length ==0) return;
			var html='';
			var intro;
			for(i in ret){
				intro =ret[i].intro;
				for(j in keywords){
					var reg =new RegExp(keywords[j],'gmi');
					intro =intro.replace(reg,'<font color=brown>'+keywords[j]+'</font>');
				}
				html =html + '[<a href=/Download/item/id/'+ret[i].new_id+'.html>'+ret[i].name+'</a>]&nbsp; '+intro+'<BR>';
			}
			$('#relate-items').append(html);
		});
		
	}
	function get_score_name(score){
		switch(score){
		case '100': return '很好，推荐下载';
		case '85': return '还不错';
		case '75': return '一般，勉强可用';
		case '50': return '差';
		case '3': return '纯粹是垃圾';
		case '40': return '和说明完全不符';
		case '20': return '文件不全';
		case '10': return '不是源代码或资料';
		case '5': return '文件有密码，不知道密码';
		case '0': return '不能解压或下载失败';
		}
		return '';
	}
	function get_comment(){
		var url ="/Download/get_comments/id/"+g_id+'.html';
		$.get(url,function(ret){
			if(ret.length ==0) return;
			var total_count =ret.total_count;
			var data =ret.data;
			var html='';
			for(i in data){
				html =html + '<a href="/User/profile/id/'+data[i].user_new_id+'.html" class="uploader">'+data[i].user_name+'</a>: <span class="comment-score">'+get_score_name(data[i].score)+'</span>, '+data[i].content+'<BR>';
			}
			$('#file-comments').append(html);
		});
		
	}
	function get_favor(){
		var url ="/Download/get_item_favors/id/"+g_id+".html";
		$.get(url,function(ret){
			if(ret.length ==0) return;
			var html='';
			for(i in ret){
				html =html + '<a href=/User/profile/id/'+ret[i].user_new_id+'.html class=user>'+ret[i].name+'</a>&nbsp;';
			}
			$('#favor-users').html(html);
		});
	}
	// 得到下载这个的用户又下载了什么
	function get_more_download(){
		
	}
	// 得到下载这个的用户又搜索了什么
	function get_more_keyword(){
		
	}
	
	// 得到论坛相关问题
	function get_bbs(){
		
	}
	
	// 得到软件工场相关内容
	function get_works(){
		
	}
	
	// 得到相关聊天室
	function get_chat(){
		
	}
	
	// 得到相关软件商城信息
	function get_shop(){
		
	}
	
	// 得到job
	
	// 得到学习内容

	// 数据
	get_download_user();
	//get_relate_item();
	get_comment();
	get_favor();
	
	$('.vote-up').click(function(){
		var url="/Download/vote/t/up/id/"+g_id;
		$.get(url,function(ret){
			if(ret.status==0){
				alert(ret.info);
				if(ret.url.length >0)
					location.href=ret.url;
			}
			else{
				var s =$('#vote-up-count').html();
				if(s =='') s="0";
				var count =parseInt(s)+1;
				$('#vote-up-count').html(count);
			}
		})
		return false;
	});
	$('.vote-down').click(function(){
		var url="/Download/vote/t/down/id/"+g_id;
		$.get(url,function(ret){
			if(ret.status==0)
				alert(ret.info);
			else{
				var s =$('#vote-down-count').html();
				if(s =='') s="0";
				var count =parseInt(s)+1;
				$('#vote-down-count').html(count);
			}
		})
		return false;
	});
	$('.favor-item').click(function(){
		var url="/Favor/add/t/0/id/"+g_id;
		$.get(url,function(ret){
			if(ret.status==0){
				alert(ret.info);
			}
			else{
				var s =$('#favor-count').html();
				if(s =='') s="0";
				var count =parseInt(s)+1;
				$('#favor-count').html(count);
			}
		})
		return false;
	});
	$('.pop-a').click(function(){
		var url=$(this).attr('href');
		$.get(url,function(ret){
			alert(ret.info);
			//location.reload();
		})
		return false;
	});

document.getElementById('readme').innerHTML =marked.parse(document.getElementById('readme').innerHTML);
</script>



</html>